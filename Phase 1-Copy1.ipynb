{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list()\n",
    "answers = list()\n",
    "answers_with_tags = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "with open('FullData.json') as js:\n",
    "    data = json.load(js)\n",
    "    print(type(data))\n",
    "    for i in data:\n",
    "        if random.randint(0,1) == 1:\n",
    "            \n",
    "            questions.append(i['patient'])\n",
    "            answers_with_tags.append(i['agent'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3317"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3317"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers_with_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 533\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import preprocessing , utils\n",
    "\n",
    "for i in range( len( answers_with_tags ) ) :\n",
    "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'start',\n",
       " 'end',\n",
       " 'you',\n",
       " 'a',\n",
       " 'pain',\n",
       " 'having',\n",
       " 'is',\n",
       " 'more',\n",
       " 'me',\n",
       " 'am',\n",
       " 'have',\n",
       " 'son',\n",
       " 'the',\n",
       " 'high',\n",
       " 'weakness',\n",
       " 'to',\n",
       " 'doctor',\n",
       " 'my',\n",
       " 'your',\n",
       " 'abdominal',\n",
       " 'typhoid',\n",
       " 'please',\n",
       " 'fever',\n",
       " 'tell',\n",
       " 'that',\n",
       " 'apetite',\n",
       " 'muscle',\n",
       " 'fatigue',\n",
       " 'and',\n",
       " 'jaundice',\n",
       " 'dengue',\n",
       " 'from',\n",
       " 'may',\n",
       " 'suffering',\n",
       " 'be',\n",
       " 'yellow',\n",
       " 'in',\n",
       " 'daughter',\n",
       " 'has',\n",
       " 'information',\n",
       " 'for',\n",
       " 'kindly',\n",
       " 'are',\n",
       " 'but',\n",
       " 'there',\n",
       " 'sweating',\n",
       " 'rash',\n",
       " 'aches',\n",
       " 'stomach',\n",
       " 'poor',\n",
       " 'constipation',\n",
       " 'headache',\n",
       " 'of',\n",
       " 'very',\n",
       " 'loss',\n",
       " 'urine',\n",
       " 'chance',\n",
       " 'get',\n",
       " 'sorry',\n",
       " 'skin',\n",
       " 'eyes',\n",
       " 'vomit',\n",
       " 'nose',\n",
       " 'pale',\n",
       " 'stools',\n",
       " 'eye',\n",
       " 'yellowish',\n",
       " 'headaches',\n",
       " 'certainly',\n",
       " 'vomiting',\n",
       " 'recommend',\n",
       " 'joint',\n",
       " 'bleeding',\n",
       " 'would',\n",
       " 'need',\n",
       " 'severe',\n",
       " 'last',\n",
       " 'about',\n",
       " 'days',\n",
       " 'few',\n",
       " 'seem',\n",
       " 'apologies',\n",
       " 'quite',\n",
       " 'thanks',\n",
       " 'lot',\n",
       " 'her',\n",
       " 'him',\n",
       " 'thank',\n",
       " 'mild',\n",
       " 'bit',\n",
       " 'help',\n",
       " 'could',\n",
       " 'with',\n",
       " 'possibly',\n",
       " 'situation',\n",
       " 'region',\n",
       " 'think',\n",
       " 'whites',\n",
       " 'abdomen',\n",
       " 'probably',\n",
       " 'give',\n",
       " 'sudden',\n",
       " 'itching',\n",
       " 'specifics',\n",
       " 'tinge',\n",
       " 'going',\n",
       " 'through',\n",
       " 'according',\n",
       " 'provide',\n",
       " 'dark',\n",
       " 'behind',\n",
       " 'his',\n",
       " 'cold',\n",
       " 'taking',\n",
       " 'common',\n",
       " 'visiting',\n",
       " 'proper',\n",
       " 'checkup',\n",
       " 'medication',\n",
       " 'consult',\n",
       " 'sure',\n",
       " 'visit',\n",
       " 'meet',\n",
       " 'itchiness',\n",
       " 'can',\n",
       " 'yourself',\n",
       " 'chances',\n",
       " 'physical',\n",
       " 'checked',\n",
       " 'hello',\n",
       " 'runny',\n",
       " 'cough',\n",
       " 'sore',\n",
       " 'throat',\n",
       " 'stuffy',\n",
       " 'good',\n",
       " 'so',\n",
       " 'out',\n",
       " 'hi',\n",
       " 'what',\n",
       " 'insufficient',\n",
       " 'morning',\n",
       " 'health',\n",
       " 'or',\n",
       " 'it',\n",
       " 'problem',\n",
       " 'how',\n",
       " 'gonna',\n",
       " 'afternoon',\n",
       " 'by',\n",
       " 'should',\n",
       " 'an',\n",
       " 'do',\n",
       " 'not',\n",
       " 'too',\n",
       " 'been',\n",
       " 'if',\n",
       " 'this',\n",
       " 'causes',\n",
       " 'much',\n",
       " 'wrong',\n",
       " 'first',\n",
       " 'avoid',\n",
       " 'myself',\n",
       " 'on',\n",
       " 'maybe',\n",
       " 'wish',\n",
       " 'know',\n",
       " 'provided',\n",
       " 'typhi',\n",
       " \"can't\",\n",
       " 'say',\n",
       " 'ravi',\n",
       " 'pragati',\n",
       " 'sick',\n",
       " 'well',\n",
       " 'vaccine',\n",
       " 'ai',\n",
       " 'mosquito',\n",
       " 'name',\n",
       " 'some',\n",
       " 'treatment',\n",
       " 'will',\n",
       " 'consider',\n",
       " 'caused',\n",
       " 'hepatitis',\n",
       " 'feeling',\n",
       " 'take',\n",
       " 'prevent',\n",
       " 'brother',\n",
       " 'water',\n",
       " 'ill',\n",
       " 'vaccines',\n",
       " 'no',\n",
       " 'salmonella',\n",
       " 'food',\n",
       " 'blood',\n",
       " 'sister',\n",
       " 'as',\n",
       " 'dehydration',\n",
       " 'sourav',\n",
       " 'feel',\n",
       " 'use',\n",
       " 'like',\n",
       " 'care',\n",
       " 'virus',\n",
       " 'shreya',\n",
       " 'fine',\n",
       " 'reason',\n",
       " 'medicines',\n",
       " 'figure',\n",
       " 'called',\n",
       " 'done',\n",
       " 'also',\n",
       " 'induced',\n",
       " 'then',\n",
       " 'contact',\n",
       " 'infected',\n",
       " 'requires',\n",
       " 'antiviral',\n",
       " 'medications',\n",
       " 'anemia',\n",
       " 'boosting',\n",
       " 'iron',\n",
       " 'prakash',\n",
       " 'rohan',\n",
       " 'shivalik',\n",
       " 'kareena',\n",
       " 'try',\n",
       " 'developing',\n",
       " 'live',\n",
       " 'drink',\n",
       " 'bilirubin',\n",
       " 'underlying',\n",
       " 'stay',\n",
       " 'spread',\n",
       " 'prevention',\n",
       " 'cure',\n",
       " 'right',\n",
       " 'recommends',\n",
       " 'away',\n",
       " 'treated',\n",
       " 'specific',\n",
       " 'shubham',\n",
       " 'atul',\n",
       " 'ashish',\n",
       " 'vikram',\n",
       " 'apurav',\n",
       " 'anamika',\n",
       " 'haritica',\n",
       " 'hari',\n",
       " 'abhishek',\n",
       " 'kartik',\n",
       " 'aayra',\n",
       " 'raj',\n",
       " 'reasons',\n",
       " 'does',\n",
       " 'bacteria',\n",
       " 's',\n",
       " 'inactivated',\n",
       " 'being',\n",
       " \"what's\",\n",
       " 'great',\n",
       " 'exist',\n",
       " 'types',\n",
       " 'one',\n",
       " 'maintain',\n",
       " 'result',\n",
       " 'production',\n",
       " 'any',\n",
       " 'disease',\n",
       " 'sahil',\n",
       " 'anand',\n",
       " 'amit',\n",
       " 'awantika',\n",
       " 'aarav',\n",
       " 'shivam',\n",
       " 'utkarsh',\n",
       " 'piyusha',\n",
       " 'pallavi',\n",
       " 'prakriti',\n",
       " 'niharika',\n",
       " 'doing',\n",
       " 'status',\n",
       " 'issue',\n",
       " 'nations',\n",
       " 'world',\n",
       " 'only',\n",
       " 'happens',\n",
       " 'liver',\n",
       " 'while',\n",
       " 'aedes',\n",
       " 'hours',\n",
       " 'since',\n",
       " 'see',\n",
       " 'why',\n",
       " 'let',\n",
       " 'refer',\n",
       " 'two',\n",
       " 'hydrated',\n",
       " 'depend',\n",
       " 'cause',\n",
       " 'steroid',\n",
       " 'amount',\n",
       " 'viruses',\n",
       " 'bites',\n",
       " 'available',\n",
       " 'countries',\n",
       " 'years',\n",
       " 'old',\n",
       " 'ananya',\n",
       " 'sakshi',\n",
       " 'harshita',\n",
       " 'pravendra',\n",
       " 'aryan',\n",
       " 'srihari',\n",
       " 'swastik',\n",
       " 'anil',\n",
       " 'purnima',\n",
       " 'rohit',\n",
       " 'anjaly',\n",
       " 'neha',\n",
       " 'unwell',\n",
       " 'we',\n",
       " 'tips',\n",
       " 'just',\n",
       " 'spreads',\n",
       " 'contaminated',\n",
       " 'close',\n",
       " 'someone',\n",
       " \"who's\",\n",
       " 'taps',\n",
       " 'clean',\n",
       " 'when',\n",
       " 'infection',\n",
       " 'healthy',\n",
       " 'person',\n",
       " 'depends',\n",
       " 'condition',\n",
       " 'mosquitoes',\n",
       " 'around',\n",
       " 'under',\n",
       " 'bitten',\n",
       " 'exists',\n",
       " 'plenty',\n",
       " 'fluids',\n",
       " 'recovering',\n",
       " 'watch',\n",
       " 'signs',\n",
       " 'symptoms',\n",
       " 'call',\n",
       " 'develop',\n",
       " 'decreased',\n",
       " 'urination',\n",
       " 'tears',\n",
       " 'dry',\n",
       " 'mouth',\n",
       " 'lips',\n",
       " 'lethargy',\n",
       " 'confusion',\n",
       " 'clammy',\n",
       " 'extremities',\n",
       " 'vaccinated',\n",
       " 'against',\n",
       " 'treat',\n",
       " 'exactly',\n",
       " 'come',\n",
       " 'place',\n",
       " 'probem',\n",
       " 'nice',\n",
       " 'hear',\n",
       " 'most',\n",
       " 'often',\n",
       " 'disorder',\n",
       " 'either',\n",
       " 'prevents',\n",
       " 'getting',\n",
       " 'rid',\n",
       " 'b',\n",
       " 'oily',\n",
       " 'however',\n",
       " 'intervention',\n",
       " 'depending',\n",
       " 'treatments',\n",
       " 'preventing',\n",
       " 'transfusion',\n",
       " 'virat',\n",
       " 'simran',\n",
       " 'time',\n",
       " 'tuesday',\n",
       " 'thankyou',\n",
       " 'problems',\n",
       " 'remains',\n",
       " 'serious',\n",
       " 'threat',\n",
       " 'especially',\n",
       " 'children',\n",
       " 'orally',\n",
       " 'via',\n",
       " 'injections',\n",
       " 'eat',\n",
       " 'pasteurized',\n",
       " 'dairy',\n",
       " 'disruption',\n",
       " 'normal',\n",
       " 'metabolism',\n",
       " 'manage',\n",
       " 'cholestrol',\n",
       " 'levels',\n",
       " 'type',\n",
       " 'aegypti',\n",
       " 'during',\n",
       " 'daytime',\n",
       " 'particularly',\n",
       " 'dawn',\n",
       " 'dusk',\n",
       " 'family',\n",
       " 'transmitted',\n",
       " 'licensed',\n",
       " 'people',\n",
       " 'ages',\n",
       " '9',\n",
       " '45',\n",
       " 'organization',\n",
       " 'given',\n",
       " 'persons',\n",
       " 'confirmed',\n",
       " 'prior',\n",
       " 'repellent',\n",
       " 'at',\n",
       " 'deet',\n",
       " 'viral',\n",
       " 'other',\n",
       " 'diseases',\n",
       " 'relievers',\n",
       " 'acetaminophen',\n",
       " 'aspirin',\n",
       " 'up',\n",
       " 'couple',\n",
       " 'monday',\n",
       " \"son's\",\n",
       " 'allow',\n",
       " 'non',\n",
       " 'bottled',\n",
       " 'wash',\n",
       " 'hands',\n",
       " 'hygiene',\n",
       " 'junk',\n",
       " 'keep',\n",
       " 'ands',\n",
       " 'drinking',\n",
       " 'open',\n",
       " 'sources',\n",
       " 'yellowing',\n",
       " 'body',\n",
       " 'process',\n",
       " 'properly',\n",
       " 'due',\n",
       " 'separate',\n",
       " 'combined',\n",
       " 'them',\n",
       " 'limit',\n",
       " 'alchohol',\n",
       " 'consumption',\n",
       " 'he',\n",
       " 'four',\n",
       " 'thrive',\n",
       " 'near',\n",
       " 'human',\n",
       " 'lodgings',\n",
       " 'enters',\n",
       " 'considered',\n",
       " 'th',\n",
       " 'ebest',\n",
       " 'solution',\n",
       " 'approved',\n",
       " 'unwise',\n",
       " 'unless',\n",
       " 'strictly',\n",
       " 'sleep',\n",
       " 'net',\n",
       " 'night',\n",
       " 'insect',\n",
       " 'products',\n",
       " 'containing',\n",
       " 'n',\n",
       " 'diethylmetatoluamide',\n",
       " 'effective',\n",
       " 'used',\n",
       " 'babies',\n",
       " 'satyam',\n",
       " 'rakesh',\n",
       " 'deepika',\n",
       " 'hey',\n",
       " 'medicine',\n",
       " '2',\n",
       " 'tht',\n",
       " 'dose',\n",
       " 'injection',\n",
       " 'oral',\n",
       " 'might',\n",
       " 'side',\n",
       " 'effects',\n",
       " 'natural',\n",
       " 'way',\n",
       " 'within',\n",
       " 'recommended',\n",
       " 'alcohol',\n",
       " 'limits',\n",
       " 'weight',\n",
       " 'cholesterol',\n",
       " 'diet',\n",
       " 'always',\n",
       " 'life',\n",
       " 'staying',\n",
       " 'concentration',\n",
       " 'make',\n",
       " 'logging',\n",
       " 'assistance',\n",
       " 'chilling',\n",
       " 'nothing',\n",
       " 'assist',\n",
       " 'elaborate',\n",
       " 'least',\n",
       " '10',\n",
       " 'percent',\n",
       " 'diethyltoluamide',\n",
       " 'higher',\n",
       " 'longer',\n",
       " 'lengths',\n",
       " 'exposure']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x1d63105c0b8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3317, 27) 27\n",
      "(3317, 68) 68\n",
      "(3317, 68, 533)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )\n",
    "\n",
    "# Saving all the arrays to storage\n",
    "np.save( 'enc_in_data.npy' , encoder_input_data )\n",
    "np.save( 'dec_in_data.npy' , decoder_input_data )\n",
    "np.save( 'dec_tar_data.npy' , decoder_output_data )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=533  # dim =200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1123 13:24:19.869920  3880 deprecation.py:506] From D:\\Anaconda\\envs\\tfGPUclone\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1123 13:24:19.921966  3880 deprecation.py:506] From D:\\Anaconda\\envs\\tfGPUclone\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1123 13:24:20.180442  3880 deprecation.py:323] From D:\\Anaconda\\envs\\tfGPUclone\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# new edited\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, dim , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( dim , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, dim , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( dim , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['categorical_crossentropy','sparse_categorical_crossentropy','kullback_leibler_divergence','poisson','cosine_proximity']\n",
    "opts = [tf.keras.optimizers.RMSprop(),tf.keras.optimizers.Adagrad()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 533)    284089      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 533)    284089      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 533), (None, 2274844     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 533),  2274844     embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 533)    284622      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,402,488\n",
      "Trainable params: 5,402,488\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3317/3317 [==============================] - 190s 57ms/sample - loss: 1.0157\n",
      "Epoch 2/100\n",
      "3317/3317 [==============================] - 25s 7ms/sample - loss: 0.7270\n",
      "Epoch 3/100\n",
      "3317/3317 [==============================] - 26s 8ms/sample - loss: 0.4601\n",
      "Epoch 4/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.2947\n",
      "Epoch 5/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.2084\n",
      "Epoch 6/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.1611\n",
      "Epoch 7/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.1298\n",
      "Epoch 8/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.1053\n",
      "Epoch 9/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0883\n",
      "Epoch 10/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0745\n",
      "Epoch 11/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0656\n",
      "Epoch 12/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0602\n",
      "Epoch 13/100\n",
      "3317/3317 [==============================] - 32s 10ms/sample - loss: 0.0556\n",
      "Epoch 14/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0529\n",
      "Epoch 15/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0498\n",
      "Epoch 16/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0479\n",
      "Epoch 17/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0464\n",
      "Epoch 18/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0495\n",
      "Epoch 19/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0428\n",
      "Epoch 20/100\n",
      "3317/3317 [==============================] - 33s 10ms/sample - loss: 0.0419\n",
      "Epoch 21/100\n",
      "3317/3317 [==============================] - 34s 10ms/sample - loss: 0.0411\n",
      "Epoch 22/100\n",
      "3317/3317 [==============================] - 34s 10ms/sample - loss: 0.0404\n",
      "Epoch 23/100\n",
      "3317/3317 [==============================] - 34s 10ms/sample - loss: 0.0395\n",
      "Epoch 24/100\n",
      "3317/3317 [==============================] - 34s 10ms/sample - loss: 0.0391\n",
      "Epoch 25/100\n",
      "3317/3317 [==============================] - 34s 10ms/sample - loss: 0.0381\n",
      "Epoch 26/100\n",
      "3317/3317 [==============================] - 34s 10ms/sample - loss: 0.0367\n",
      "Epoch 27/100\n",
      "3317/3317 [==============================] - 34s 10ms/sample - loss: 0.0365\n",
      "Epoch 28/100\n",
      "3317/3317 [==============================] - 34s 10ms/sample - loss: 0.0360\n",
      "Epoch 29/100\n",
      "3317/3317 [==============================] - 37s 11ms/sample - loss: 0.0358\n",
      "Epoch 30/100\n",
      "3317/3317 [==============================] - 37s 11ms/sample - loss: 0.0357\n",
      "Epoch 31/100\n",
      "3317/3317 [==============================] - 37s 11ms/sample - loss: 0.0339\n",
      "Epoch 32/100\n",
      "3317/3317 [==============================] - 36s 11ms/sample - loss: 0.0340\n",
      "Epoch 33/100\n",
      "3317/3317 [==============================] - 36s 11ms/sample - loss: 0.0333\n",
      "Epoch 34/100\n",
      "3317/3317 [==============================] - 37s 11ms/sample - loss: 0.0332\n",
      "Epoch 35/100\n",
      "3317/3317 [==============================] - 36s 11ms/sample - loss: 0.0325\n",
      "Epoch 36/100\n",
      "3317/3317 [==============================] - 36s 11ms/sample - loss: 0.0325\n",
      "Epoch 37/100\n",
      "3317/3317 [==============================] - 36s 11ms/sample - loss: 0.0316\n",
      "Epoch 38/100\n",
      "3317/3317 [==============================] - 34s 10ms/sample - loss: 0.0315\n",
      "Epoch 39/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0309\n",
      "Epoch 40/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0303\n",
      "Epoch 41/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0296\n",
      "Epoch 42/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0293\n",
      "Epoch 43/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0288\n",
      "Epoch 44/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0279\n",
      "Epoch 45/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0273\n",
      "Epoch 46/100\n",
      "3317/3317 [==============================] - 32s 10ms/sample - loss: 0.0268\n",
      "Epoch 47/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0260\n",
      "Epoch 48/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0251\n",
      "Epoch 49/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0243\n",
      "Epoch 50/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0238\n",
      "Epoch 51/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0225\n",
      "Epoch 52/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0217\n",
      "Epoch 53/100\n",
      "3317/3317 [==============================] - 33s 10ms/sample - loss: 0.0211\n",
      "Epoch 54/100\n",
      "3317/3317 [==============================] - 32s 10ms/sample - loss: 0.0201\n",
      "Epoch 55/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0193\n",
      "Epoch 56/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0183\n",
      "Epoch 57/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0174\n",
      "Epoch 58/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0165\n",
      "Epoch 59/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0156\n",
      "Epoch 60/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0146\n",
      "Epoch 61/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0140\n",
      "Epoch 62/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0133\n",
      "Epoch 63/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0124\n",
      "Epoch 64/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0114\n",
      "Epoch 65/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0109\n",
      "Epoch 66/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0102\n",
      "Epoch 67/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0094\n",
      "Epoch 68/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0090\n",
      "Epoch 69/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0084\n",
      "Epoch 70/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0085\n",
      "Epoch 71/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0073\n",
      "Epoch 72/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0074\n",
      "Epoch 73/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0067\n",
      "Epoch 74/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0064\n",
      "Epoch 75/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0061\n",
      "Epoch 76/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0058\n",
      "Epoch 77/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0054\n",
      "Epoch 78/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0054\n",
      "Epoch 79/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0050\n",
      "Epoch 80/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0052\n",
      "Epoch 81/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0044\n",
      "Epoch 82/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0047\n",
      "Epoch 83/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0043\n",
      "Epoch 84/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0042\n",
      "Epoch 85/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0041\n",
      "Epoch 86/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0036\n",
      "Epoch 87/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0037\n",
      "Epoch 88/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0036\n",
      "Epoch 89/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0034\n",
      "Epoch 90/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0034\n",
      "Epoch 91/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0034\n",
      "Epoch 92/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0032\n",
      "Epoch 93/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0031\n",
      "Epoch 94/100\n",
      "3317/3317 [==============================] - 29s 9ms/sample - loss: 0.0031\n",
      "Epoch 95/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0029\n",
      "Epoch 96/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0029\n",
      "Epoch 97/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0031\n",
      "Epoch 98/100\n",
      "3317/3317 [==============================] - 30s 9ms/sample - loss: 0.0023\n",
      "Epoch 99/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0033\n",
      "Epoch 100/100\n",
      "3317/3317 [==============================] - 31s 9ms/sample - loss: 0.0023\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=150, epochs=100 ) \n",
    "model.save( 'model3.h5' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( dim ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( dim ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "def solve(ex):\n",
    "    \n",
    "    sent = preprocess(ex)\n",
    "    pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    cs = cp.parse(sent)\n",
    "    #print(cs)\n",
    "    from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "    from pprint import pprint\n",
    "    iob_tagged = tree2conlltags(cs)\n",
    "    #pprint(iob_tagged)\n",
    "    ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(ex)))\n",
    "    a=str(ne_tree)\n",
    "    s=\"\"\n",
    "    n=a.find(\"PERSON\")\n",
    "    i=n+7\n",
    "    if n!=-1:\n",
    "        while a[i]!=\"/\":\n",
    "            s=s+a[i]\n",
    "            i+=1\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSymptoms(text,listOfSymptoms):\n",
    "    listOfSymptoms=[x.lower() for x in listOfSymptoms]\n",
    "    bag=[]\n",
    "    for item in listOfSymptoms:\n",
    "        if item in text.lower():\n",
    "            bag.append(item)\n",
    "            \n",
    "    return bag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfSymptoms=[\"high fever\" , \"weakness\" ,\"constipation\" , \"headache\" , \"poor apetite\" , \"stomach pain\" , \"fatigue\" , \"rash\",\n",
    "                \"abdominal pain\" ,\"sweating\" ,\"muscle aches\",\n",
    "                \n",
    "                \"sore throat\",\"runny nose\",\"stuffy nose\",\"cough\",\"mild fever\",\n",
    "                \n",
    "                \"a yellow tinge to the skin\",\"yellow skin \",\"yellowish skin\",\"pale stools\",\"a yellow tinge to the eye\",\"yellow eye whites \",\"yellowish eye whites\",\n",
    "                \"dark urine\",\"yellow urine\",\"dark yellow urine\",\"yellowish urine\",\n",
    "                \"itchiness\",\"itching\",\n",
    "                \"vomiting\",\"vomit\",\"abdominal pain\",\"pain the the abdomen\",\"pain in abdominal region\",\n",
    "                \"weakness\",\n",
    "                \"loss of apetite\",\"apetite loss\",\n",
    "                \n",
    "                \"Sudden, high fever\",\"sudden fever\",\"high fever\",\n",
    "                \"severe headaches\",\"headaches\",\n",
    "                \"abdominal pain\",\"pain in the abdomen\",\"pain in abdominal region\",\n",
    "                \"weakness\",\n",
    "                \"Pain behind the eyes\",\"pain in eyes\",\n",
    "                \"Severe joint and muscle pain\",\"joint and muscle pain\",\n",
    "                \"Fatigue\",\n",
    "                \"vomiting\",\"vomit\",\n",
    "                \"mild bleeding\",\"nose bleeding\"\n",
    "\n",
    "               \n",
    "               \n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendPatch(prevSent,lst):\n",
    "    for i in lst:\n",
    "        if i not in prevSent:\n",
    "            \n",
    "            prevSent = prevSent + \" , \"+i+\" , \"\n",
    "    prevSent = prevSent.rstrip(\", \")\n",
    "    return prevSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub(r\"i'm\",\"i am\",text)\n",
    "    text=re.sub(r\"I'm\",\"I am\",text)\n",
    "    text=re.sub(r\"he's\",\"he is\",text)\n",
    "    text=re.sub(r\"He's\",\"He is\",text)\n",
    "    text=re.sub(r\"she's\",\"she is\",text)\n",
    "    text=re.sub(r\"She's\",\"She is\",text)\n",
    "    text=re.sub(r\"that's\",\"that is\",text)\n",
    "    text=re.sub(r\"That's\",\"That is\",text)\n",
    "    text=re.sub(r\"what's\",\"what is\",text)\n",
    "    text=re.sub(r\"What's\",\"What is\",text)\n",
    "    text=re.sub(r\"where's\",\"where is\",text)\n",
    "    text=re.sub(r\"Where's\",\"Where is\",text)\n",
    "    \n",
    "    text=re.sub(r\"\\'ll\",\" will\",text)\n",
    "    text=re.sub(r\"\\'s\",\" is\",text)\n",
    "    text=re.sub(r\"\\'m\",\" am\",text)\n",
    "    \n",
    "    text=re.sub(r\"\\'ve\",\" have\",text)\n",
    "    text=re.sub(r\"\\'re\",\" are\",text)\n",
    "    text=re.sub(r\"\\'d\",\" would\",text)\n",
    "    text=re.sub(r\"won't\",\" will not\",text)\n",
    "    text=re.sub(r\"can't\",\" can not\",text)\n",
    "    text=re.sub(r\"[-()\\\"#$/?|.<>@:;+=,]\",\"\",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User : What's going on\n",
      "what is going on\n",
      "Agent : What is going on\n",
      "User : What's up\n",
      "what is up\n",
      "Agent : Dengue is spread by a type of infected mosquito called the aedes aegypti mosquito the mosquito bites during daytime hours particularly around the hours of dawn and dusk\n",
      "User : /stop\n",
      "stop\n",
      "Agent : You should manage your cholestrol levels and maintain a healthy diet also stay hydrated always\n",
      "User : /stop\n",
      "stop\n",
      "Agent : You should manage your cholestrol levels and maintain a healthy diet also stay hydrated always\n",
      "User : /stop\n",
      "stop\n",
      "Agent : You should manage your cholestrol levels and maintain a healthy diet also stay hydrated always\n"
     ]
    }
   ],
   "source": [
    "user_data={}\n",
    "bag=[]\n",
    "\n",
    "while True:\n",
    "    \n",
    "    a=input( 'User : ' )\n",
    "    if a == \"/stop\":\n",
    "        break\n",
    "        \n",
    "    a=clean_text(a)\n",
    "    a_1=a.lower()\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    if a_1 in [\"hi\",\"hi there\",\"hello\",\"hola\",\"well hello there\"]:\n",
    "        print( \"Agent : \"+ \" Hi . What's your name ?\" )\n",
    "        continue\n",
    "    \n",
    "    if a_1 in [\"thanks\",\"thank you\",\"thankyou\",\"thanks a lot\",\"thankyou very much\",\"thank you very much\"]:\n",
    "        print( \"Agent : \"+ \" Hi . What's your name ?\" )\n",
    "        continue\n",
    "    \n",
    "    s=solve(a)\n",
    "    if s!=\"\":\n",
    "        user_data[\"name\"]=s\n",
    "        print(\"Agent : Hi \"+ s + \" ! How can I help you ..\")\n",
    "        continue\n",
    "        \n",
    "    bag.extend(extractSymptoms(a_1,listOfSymptoms))\n",
    "    a_1 = appendPatch(a_1,bag)\n",
    "    \n",
    "    ls=a_1.strip().split()\n",
    "    \n",
    "    for i in ls :\n",
    "        if i not in list(dic.keys()):\n",
    "            a_1=a_1.replace(i,\" \")\n",
    "            \n",
    "    states_values = enc_model.predict( str_to_tokens(a_1 ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "    deco=decoded_translation.strip('end')\n",
    "    \n",
    "    print( \"Agent : \"+ deco.strip().capitalize() )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sore throat', 'runny nose', 'cough', 'mild fever']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample login demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    print(\"Do you have an account .. (Y/n)\")\n",
    "    a=input().lower()\n",
    "    if a == \"n\":\n",
    "        print(\"Sign up to access our assistant ... \")\n",
    "        print(\"Pick a username ..\")\n",
    "        us_name=input()\n",
    "        print(\"Password please\")\n",
    "        password=input()\n",
    "        if us_name not in list(users.keys()):\n",
    "            users[us_name]=password\n",
    "        else:\n",
    "            print(\"Sorry but it is taken \")\n",
    "\n",
    "\n",
    "\n",
    "    elif a == \"y\":\n",
    "        print(\"Give your Username\")\n",
    "        us_name=input()\n",
    "        print(\"Password\")  \n",
    "        password=input()\n",
    "        if us_name not in list(users.keys()):\n",
    "            print(\"Please sign up\")\n",
    "        elif users[us_name]==password:\n",
    "            print(\"Validated Successfully\")\n",
    "            break\n",
    "        elif users[us_name] != password:\n",
    "            print(\"Wrong password\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Typo .. \")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
