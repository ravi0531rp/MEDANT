{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to our Chatbot \"MEDANT 1.0\"\n",
    "It is a semi context-aware medical assistant. Let's go through it.\n",
    "<img src=\"https://icon-library.net/images/bot-icon/bot-icon-6.jpg\" width=\"500\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In order to build the Chatbot, we need some helper libraries and frameworks\n",
    "### Let's import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list()\n",
    "answers = list()\n",
    "answers_with_tags = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "with open('FullData.json') as js:\n",
    "    data = json.load(js)\n",
    "    print(type(data))\n",
    "    for i in data:\n",
    "        \n",
    "            \n",
    "        questions.append(i['patient'])\n",
    "        answers_with_tags.append(i['agent'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6574"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6574"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(answers_with_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 553\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import preprocessing , utils\n",
    "\n",
    "for i in range( len( answers_with_tags ) ) :\n",
    "    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'start',\n",
       " 'end',\n",
       " 'a',\n",
       " 'you',\n",
       " 'pain',\n",
       " 'having',\n",
       " 'is',\n",
       " 'more',\n",
       " 'me',\n",
       " 'am',\n",
       " 'have',\n",
       " 'son',\n",
       " 'the',\n",
       " 'high',\n",
       " 'weakness',\n",
       " 'doctor',\n",
       " 'to',\n",
       " 'my',\n",
       " 'your',\n",
       " 'abdominal',\n",
       " 'please',\n",
       " 'fever',\n",
       " 'typhoid',\n",
       " 'tell',\n",
       " 'that',\n",
       " 'apetite',\n",
       " 'muscle',\n",
       " 'fatigue',\n",
       " 'and',\n",
       " 'dengue',\n",
       " 'jaundice',\n",
       " 'from',\n",
       " 'suffering',\n",
       " 'yellow',\n",
       " 'be',\n",
       " 'may',\n",
       " 'in',\n",
       " 'daughter',\n",
       " 'has',\n",
       " 'for',\n",
       " 'information',\n",
       " 'kindly',\n",
       " 'are',\n",
       " 'there',\n",
       " 'but',\n",
       " 'constipation',\n",
       " 'headache',\n",
       " 'poor',\n",
       " 'stomach',\n",
       " 'rash',\n",
       " 'sweating',\n",
       " 'aches',\n",
       " 'of',\n",
       " 'very',\n",
       " 'urine',\n",
       " 'loss',\n",
       " 'chance',\n",
       " 'skin',\n",
       " 'get',\n",
       " 'vomit',\n",
       " 'certainly',\n",
       " 'eye',\n",
       " 'yellowish',\n",
       " 'pale',\n",
       " 'stools',\n",
       " 'joint',\n",
       " 'eyes',\n",
       " 'nose',\n",
       " 'recommend',\n",
       " 'sorry',\n",
       " 'would',\n",
       " 'severe',\n",
       " 'headaches',\n",
       " 'vomiting',\n",
       " 'bleeding',\n",
       " 'apologies',\n",
       " 'quite',\n",
       " 'last',\n",
       " 'days',\n",
       " 'thanks',\n",
       " 'lot',\n",
       " 'her',\n",
       " 'about',\n",
       " 'few',\n",
       " 'need',\n",
       " 'thank',\n",
       " 'him',\n",
       " 'seem',\n",
       " 'help',\n",
       " 'bit',\n",
       " 'mild',\n",
       " 'could',\n",
       " 'with',\n",
       " 'situation',\n",
       " 'possibly',\n",
       " 'whites',\n",
       " 'region',\n",
       " 'sudden',\n",
       " 'probably',\n",
       " 'give',\n",
       " 'abdomen',\n",
       " 'think',\n",
       " 'going',\n",
       " 'through',\n",
       " 'itching',\n",
       " 'tinge',\n",
       " 'specifics',\n",
       " 'dark',\n",
       " 'according',\n",
       " 'provide',\n",
       " 'taking',\n",
       " 'proper',\n",
       " 'checkup',\n",
       " 'medication',\n",
       " 'sure',\n",
       " 'his',\n",
       " 'consult',\n",
       " 'behind',\n",
       " 'can',\n",
       " 'meet',\n",
       " 'visiting',\n",
       " 'cold',\n",
       " 'common',\n",
       " 'itchiness',\n",
       " 'visit',\n",
       " 'yourself',\n",
       " 'chances',\n",
       " 'physical',\n",
       " 'checked',\n",
       " 'so',\n",
       " 'hello',\n",
       " 'out',\n",
       " 'sore',\n",
       " 'throat',\n",
       " 'runny',\n",
       " 'stuffy',\n",
       " 'cough',\n",
       " 'good',\n",
       " 'hi',\n",
       " 'what',\n",
       " 'health',\n",
       " 'insufficient',\n",
       " 'or',\n",
       " 'morning',\n",
       " 'afternoon',\n",
       " 'it',\n",
       " 'how',\n",
       " 'problem',\n",
       " 'gonna',\n",
       " 'by',\n",
       " 'this',\n",
       " 'not',\n",
       " 'wrong',\n",
       " 'if',\n",
       " 'should',\n",
       " 'an',\n",
       " 'much',\n",
       " 'been',\n",
       " 'do',\n",
       " 'mosquito',\n",
       " \"can't\",\n",
       " 'say',\n",
       " 'wish',\n",
       " 'know',\n",
       " 'too',\n",
       " 'avoid',\n",
       " 'provided',\n",
       " 'sick',\n",
       " 'causes',\n",
       " 'ai',\n",
       " 'consider',\n",
       " 'on',\n",
       " 'will',\n",
       " 'vaccine',\n",
       " 'first',\n",
       " 'name',\n",
       " 'myself',\n",
       " 'well',\n",
       " 'some',\n",
       " 'prevent',\n",
       " 'maybe',\n",
       " 'ravi',\n",
       " 'sourav',\n",
       " 'pragati',\n",
       " 'caused',\n",
       " 'typhi',\n",
       " 'vaccines',\n",
       " 'hepatitis',\n",
       " 'feel',\n",
       " 'take',\n",
       " 'feeling',\n",
       " 'ill',\n",
       " 'treatment',\n",
       " 'no',\n",
       " 'food',\n",
       " 'water',\n",
       " 'virus',\n",
       " 'use',\n",
       " 'sister',\n",
       " 'brother',\n",
       " 'as',\n",
       " 'stay',\n",
       " 'blood',\n",
       " 'dehydration',\n",
       " 'reason',\n",
       " 'mosquitoes',\n",
       " 'try',\n",
       " 'care',\n",
       " 'spread',\n",
       " 'called',\n",
       " 'salmonella',\n",
       " 'infected',\n",
       " 'bilirubin',\n",
       " 'like',\n",
       " 'one',\n",
       " 'does',\n",
       " 'cure',\n",
       " 'medicines',\n",
       " 'also',\n",
       " 'away',\n",
       " 'prevention',\n",
       " 'recommends',\n",
       " 'live',\n",
       " 'fine',\n",
       " 'figure',\n",
       " 'done',\n",
       " 'contact',\n",
       " 'types',\n",
       " 'since',\n",
       " 'unwell',\n",
       " 'then',\n",
       " 'right',\n",
       " 'drink',\n",
       " 'maintain',\n",
       " 'induced',\n",
       " 'requires',\n",
       " 'antiviral',\n",
       " 'medications',\n",
       " 'anemia',\n",
       " 'treated',\n",
       " 'boosting',\n",
       " 'iron',\n",
       " 'being',\n",
       " 'specific',\n",
       " 'underlying',\n",
       " 'any',\n",
       " 'disease',\n",
       " 'reasons',\n",
       " 'exist',\n",
       " 'bacteria',\n",
       " 's',\n",
       " 'developing',\n",
       " 'inactivated',\n",
       " 'happens',\n",
       " 'when',\n",
       " 'liver',\n",
       " 'result',\n",
       " 'production',\n",
       " 'viruses',\n",
       " 'bites',\n",
       " 'aedes',\n",
       " 'hours',\n",
       " \"what's\",\n",
       " 'only',\n",
       " 'around',\n",
       " 'why',\n",
       " 'world',\n",
       " 'healthy',\n",
       " 'hydrated',\n",
       " 'while',\n",
       " 'hari',\n",
       " 'shubham',\n",
       " 'satyam',\n",
       " 'pravendra',\n",
       " 'abhishek',\n",
       " 'prakash',\n",
       " 'aryan',\n",
       " 'rohan',\n",
       " 'kartik',\n",
       " 'virat',\n",
       " 'rohit',\n",
       " 'sahil',\n",
       " 'sanil',\n",
       " 'srihari',\n",
       " 'raj',\n",
       " 'shivalik',\n",
       " 'swastik',\n",
       " 'anil',\n",
       " 'rakesh',\n",
       " 'anand',\n",
       " 'atul',\n",
       " 'ashish',\n",
       " 'vikram',\n",
       " 'amit',\n",
       " 'apurav',\n",
       " 'utkarsh',\n",
       " 'niharika',\n",
       " 'shreya',\n",
       " 'piyusha',\n",
       " 'pallavi',\n",
       " 'aayra',\n",
       " 'deepika',\n",
       " 'ananya',\n",
       " 'kareena',\n",
       " 'anjaly',\n",
       " 'awantika',\n",
       " 'neha',\n",
       " 'sakshi',\n",
       " 'prakriti',\n",
       " 'anamika',\n",
       " 'purnima',\n",
       " 'harshita',\n",
       " 'haritica',\n",
       " 'simran',\n",
       " 'aarav',\n",
       " 'shivam',\n",
       " 'great',\n",
       " 'person',\n",
       " 'doing',\n",
       " 'status',\n",
       " 'see',\n",
       " 'we',\n",
       " 'against',\n",
       " 'tips',\n",
       " 'just',\n",
       " 'exactly',\n",
       " 'problems',\n",
       " 'b',\n",
       " 'available',\n",
       " 'countries',\n",
       " 'two',\n",
       " 'infection',\n",
       " 'years',\n",
       " 'old',\n",
       " 'time',\n",
       " 'couple',\n",
       " 'monday',\n",
       " 'tuesday',\n",
       " 'treat',\n",
       " 'let',\n",
       " 'refer',\n",
       " 'come',\n",
       " 'place',\n",
       " 'probem',\n",
       " 'taps',\n",
       " 'eat',\n",
       " 'pasteurized',\n",
       " 'dairy',\n",
       " 'clean',\n",
       " 'manage',\n",
       " 'cholestrol',\n",
       " 'levels',\n",
       " 'oily',\n",
       " 'depend',\n",
       " 'cause',\n",
       " 'steroid',\n",
       " 'amount',\n",
       " 'depends',\n",
       " 'condition',\n",
       " 'make',\n",
       " 'repellent',\n",
       " 'at',\n",
       " 'concentration',\n",
       " 'deet',\n",
       " 'under',\n",
       " 'bitten',\n",
       " 'however',\n",
       " 'intervention',\n",
       " 'depending',\n",
       " 'treatments',\n",
       " 'preventing',\n",
       " 'transfusion',\n",
       " 'viral',\n",
       " 'other',\n",
       " 'diseases',\n",
       " 'relievers',\n",
       " 'acetaminophen',\n",
       " 'aspirin',\n",
       " 'exists',\n",
       " 'plenty',\n",
       " 'fluids',\n",
       " 'recovering',\n",
       " 'watch',\n",
       " 'signs',\n",
       " 'symptoms',\n",
       " 'call',\n",
       " 'develop',\n",
       " 'decreased',\n",
       " 'urination',\n",
       " 'tears',\n",
       " 'dry',\n",
       " 'mouth',\n",
       " 'lips',\n",
       " 'lethargy',\n",
       " 'confusion',\n",
       " 'clammy',\n",
       " 'extremities',\n",
       " 'spreads',\n",
       " 'contaminated',\n",
       " 'close',\n",
       " 'someone',\n",
       " \"who's\",\n",
       " 'remains',\n",
       " 'serious',\n",
       " 'threat',\n",
       " 'especially',\n",
       " 'children',\n",
       " 'issue',\n",
       " 'nations',\n",
       " 'yellowing',\n",
       " 'body',\n",
       " 'process',\n",
       " 'properly',\n",
       " 'due',\n",
       " 'most',\n",
       " 'often',\n",
       " 'disorder',\n",
       " 'either',\n",
       " 'prevents',\n",
       " 'getting',\n",
       " 'rid',\n",
       " 'disruption',\n",
       " 'normal',\n",
       " 'metabolism',\n",
       " 'four',\n",
       " 'thrive',\n",
       " 'near',\n",
       " 'human',\n",
       " 'lodgings',\n",
       " 'enters',\n",
       " 'family',\n",
       " 'transmitted',\n",
       " 'type',\n",
       " 'aegypti',\n",
       " 'during',\n",
       " 'daytime',\n",
       " 'particularly',\n",
       " 'dawn',\n",
       " 'dusk',\n",
       " 'people',\n",
       " 'vaccinated',\n",
       " 'thankyou',\n",
       " '2',\n",
       " 'tht',\n",
       " 'dose',\n",
       " 'injection',\n",
       " 'oral',\n",
       " 'orally',\n",
       " 'via',\n",
       " 'injections',\n",
       " 'might',\n",
       " 'side',\n",
       " 'effects',\n",
       " 'natural',\n",
       " 'way',\n",
       " 'separate',\n",
       " 'combined',\n",
       " 'them',\n",
       " 'licensed',\n",
       " 'ages',\n",
       " '9',\n",
       " '45',\n",
       " 'organization',\n",
       " 'given',\n",
       " 'persons',\n",
       " 'confirmed',\n",
       " 'prior',\n",
       " 'considered',\n",
       " 'th',\n",
       " 'ebest',\n",
       " 'solution',\n",
       " 'approved',\n",
       " 'unwise',\n",
       " 'unless',\n",
       " 'strictly',\n",
       " 'medicine',\n",
       " \"son's\",\n",
       " 'nice',\n",
       " 'hear',\n",
       " 'allow',\n",
       " 'non',\n",
       " 'bottled',\n",
       " 'wash',\n",
       " 'hands',\n",
       " 'drinking',\n",
       " 'open',\n",
       " 'sources',\n",
       " 'hygiene',\n",
       " 'junk',\n",
       " 'keep',\n",
       " 'ands',\n",
       " 'within',\n",
       " 'recommended',\n",
       " 'alcohol',\n",
       " 'limits',\n",
       " 'weight',\n",
       " 'cholesterol',\n",
       " 'diet',\n",
       " 'always',\n",
       " 'life',\n",
       " 'staying',\n",
       " 'limit',\n",
       " 'alchohol',\n",
       " 'consumption',\n",
       " 'he',\n",
       " 'logging',\n",
       " 'least',\n",
       " '10',\n",
       " 'percent',\n",
       " 'diethyltoluamide',\n",
       " 'higher',\n",
       " 'longer',\n",
       " 'lengths',\n",
       " 'exposure',\n",
       " 'sleep',\n",
       " 'net',\n",
       " 'night',\n",
       " 'insect',\n",
       " 'products',\n",
       " 'containing',\n",
       " 'n',\n",
       " 'diethylmetatoluamide',\n",
       " 'effective',\n",
       " 'used',\n",
       " 'babies',\n",
       " 'using',\n",
       " 'netting',\n",
       " 'areas',\n",
       " 'many',\n",
       " 'windows',\n",
       " 'doors',\n",
       " 'screens',\n",
       " 'closed',\n",
       " 'allowing',\n",
       " 'into',\n",
       " 'inclosed',\n",
       " 'spaces',\n",
       " 'up',\n",
       " 'elaborate',\n",
       " 'amazing',\n",
       " 'helping',\n",
       " 'hey',\n",
       " 'needs',\n",
       " 'assistance',\n",
       " 'nothing',\n",
       " 'assist',\n",
       " 'understand',\n",
       " 'ralph',\n",
       " 'peggy',\n",
       " 'chilling',\n",
       " 'others']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x2550ff21da0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6574, 27) 27\n",
      "(6574, 68) 68\n",
      "(6574, 68, 553)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen_questions , padding='post' )\n",
    "encoder_input_data = np.array( padded_questions )\n",
    "print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )\n",
    "\n",
    "# Saving all the arrays to storage\n",
    "np.save( 'enc_in_data.npy' , encoder_input_data )\n",
    "np.save( 'dec_in_data.npy' , decoder_input_data )\n",
    "np.save( 'dec_tar_data.npy' , decoder_output_data )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=553  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1217 08:05:30.605082  1936 deprecation.py:506] From D:\\Anaconda\\envs\\tfGPUclone\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1217 08:05:30.782609  1936 deprecation.py:506] From D:\\Anaconda\\envs\\tfGPUclone\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1217 08:05:31.121701  1936 deprecation.py:323] From D:\\Anaconda\\envs\\tfGPUclone\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py:3794: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# new edited\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, dim , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( dim , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, dim , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( dim , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = ['categorical_crossentropy','sparse_categorical_crossentropy','kullback_leibler_divergence','poisson','cosine_proximity']\n",
    "opts = [tf.keras.optimizers.RMSprop(),tf.keras.optimizers.Adagrad()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 553)    305809      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 553)    305809      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 553), (None, 2448684     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 553),  2448684     embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 553)    306362      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,815,348\n",
      "Trainable params: 5,815,348\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x2550fcbb6d8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "6574/6574 [==============================] - 63s 10ms/sample - loss: 0.8776\n",
      "Epoch 2/80\n",
      "6574/6574 [==============================] - 45s 7ms/sample - loss: 0.3758\n",
      "Epoch 3/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.1894\n",
      "Epoch 4/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.1237\n",
      "Epoch 5/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0887\n",
      "Epoch 6/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0661\n",
      "Epoch 7/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0565\n",
      "Epoch 8/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0509\n",
      "Epoch 9/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0474\n",
      "Epoch 10/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0460\n",
      "Epoch 11/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0428\n",
      "Epoch 12/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0415\n",
      "Epoch 13/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0400\n",
      "Epoch 14/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0388\n",
      "Epoch 15/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0381\n",
      "Epoch 16/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0371\n",
      "Epoch 17/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0364\n",
      "Epoch 18/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0360\n",
      "Epoch 19/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0355\n",
      "Epoch 20/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0352\n",
      "Epoch 21/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0361\n",
      "Epoch 22/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0343\n",
      "Epoch 23/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0345\n",
      "Epoch 24/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0342\n",
      "Epoch 25/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0338\n",
      "Epoch 26/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0334\n",
      "Epoch 27/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0332\n",
      "Epoch 28/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0328\n",
      "Epoch 29/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0323\n",
      "Epoch 30/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0319\n",
      "Epoch 31/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0313\n",
      "Epoch 32/80\n",
      "6574/6574 [==============================] - 47s 7ms/sample - loss: 0.0309\n",
      "Epoch 33/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0302\n",
      "Epoch 34/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0294\n",
      "Epoch 35/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0287\n",
      "Epoch 36/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0279\n",
      "Epoch 37/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0269\n",
      "Epoch 38/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0259\n",
      "Epoch 39/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0248\n",
      "Epoch 40/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0236\n",
      "Epoch 41/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0224\n",
      "Epoch 42/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0211\n",
      "Epoch 43/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0199\n",
      "Epoch 44/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0185\n",
      "Epoch 45/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0171\n",
      "Epoch 46/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0160\n",
      "Epoch 47/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0148\n",
      "Epoch 48/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0135\n",
      "Epoch 49/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0126\n",
      "Epoch 50/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0117\n",
      "Epoch 51/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0108\n",
      "Epoch 52/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0099\n",
      "Epoch 53/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0091\n",
      "Epoch 54/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0086\n",
      "Epoch 55/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0081\n",
      "Epoch 56/80\n",
      "6574/6574 [==============================] - 49s 7ms/sample - loss: 0.0076\n",
      "Epoch 57/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0069\n",
      "Epoch 58/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0066\n",
      "Epoch 59/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0061\n",
      "Epoch 60/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0058\n",
      "Epoch 61/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0055\n",
      "Epoch 62/80\n",
      "6574/6574 [==============================] - 49s 7ms/sample - loss: 0.0052\n",
      "Epoch 63/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0050\n",
      "Epoch 64/80\n",
      "6574/6574 [==============================] - 49s 7ms/sample - loss: 0.0048\n",
      "Epoch 65/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0046\n",
      "Epoch 66/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0043\n",
      "Epoch 67/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0042\n",
      "Epoch 68/80\n",
      "6574/6574 [==============================] - 48s 7ms/sample - loss: 0.0041\n",
      "Epoch 69/80\n",
      "6574/6574 [==============================] - 50s 8ms/sample - loss: 0.0041\n",
      "Epoch 70/80\n",
      "6574/6574 [==============================] - 49s 7ms/sample - loss: 0.0039\n",
      "Epoch 71/80\n",
      "6574/6574 [==============================] - 50s 8ms/sample - loss: 0.0038\n",
      "Epoch 72/80\n",
      "6574/6574 [==============================] - 49s 7ms/sample - loss: 0.0037\n",
      "Epoch 73/80\n",
      "6574/6574 [==============================] - 50s 8ms/sample - loss: 0.0036\n",
      "Epoch 74/80\n",
      "6574/6574 [==============================] - 49s 8ms/sample - loss: 0.0035\n",
      "Epoch 75/80\n",
      "6574/6574 [==============================] - 49s 8ms/sample - loss: 0.0036\n",
      "Epoch 76/80\n",
      "6574/6574 [==============================] - 49s 7ms/sample - loss: 0.0033\n",
      "Epoch 77/80\n",
      "6574/6574 [==============================] - 49s 7ms/sample - loss: 0.0036\n",
      "Epoch 78/80\n",
      "6574/6574 [==============================] - 49s 8ms/sample - loss: 0.0034\n",
      "Epoch 79/80\n",
      "6574/6574 [==============================] - 49s 7ms/sample - loss: 0.0032\n",
      "Epoch 80/80\n",
      "6574/6574 [==============================] - 49s 8ms/sample - loss: 0.0033\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=150, epochs=80 ) \n",
    "model.save( 'model3.h5' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( dim ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( dim ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model , dec_model = make_inference_models()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "def solve(ex):\n",
    "    \n",
    "    sent = preprocess(ex)\n",
    "    pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "    cp = nltk.RegexpParser(pattern)\n",
    "    cs = cp.parse(sent)\n",
    "    #print(cs)\n",
    "    from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "    from pprint import pprint\n",
    "    iob_tagged = tree2conlltags(cs)\n",
    "    #pprint(iob_tagged)\n",
    "    ne_tree = nltk.ne_chunk(pos_tag(word_tokenize(ex)))\n",
    "    a=str(ne_tree)\n",
    "    s=\"\"\n",
    "    n=a.find(\"PERSON\")\n",
    "    i=n+7\n",
    "    if n!=-1:\n",
    "        while a[i]!=\"/\":\n",
    "            s=s+a[i]\n",
    "            i+=1\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSymptoms(text,listOfSymptoms):\n",
    "    listOfSymptoms=[x.lower() for x in listOfSymptoms]\n",
    "    bag=[]\n",
    "    for item in listOfSymptoms:\n",
    "        if item in text.lower():\n",
    "            bag.append(item)\n",
    "            \n",
    "    return bag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfSymptoms=[\"high fever\" , \"weakness\" ,\"constipation\" , \"headache\" , \"poor apetite\" , \"stomach pain\" , \"fatigue\" , \"rash\",\n",
    "                \"abdominal pain\" ,\"sweating\" ,\"muscle aches\",\"head is aching\",\"muscles are aching\",\"sweat\",\"rashes\"\n",
    "                \n",
    "                \"sore throat\",\"runny nose\",\"stuffy nose\",\"cough\",\"mild fever\",\"fever\",\"watery nose\"\n",
    "                \n",
    "                \"a yellow tinge to the skin\",\"yellow skin \",\"yellowish skin\",\"pale stools\",\"a yellow tinge to the eye\",\"yellow eye whites \",\"yellowish eye whites\",\n",
    "                \"dark urine\",\"yellow urine\",\"dark yellow urine\",\"yellowish urine\",\n",
    "                \"itchiness\",\"itching\",\n",
    "                \"vomiting\",\"vomit\",\"abdominal pain\",\"pain the the abdomen\",\"pain in abdominal region\",\n",
    "                \"weakness\",\n",
    "                \"loss of apetite\",\"apetite loss\",\n",
    "                \n",
    "                \"Sudden, high fever\",\"sudden fever\",\"high fever\",\n",
    "                \"severe headaches\",\"headaches\",\n",
    "                \"abdominal pain\",\"pain in the abdomen\",\"pain in abdominal region\",\n",
    "                \"weakness\",\n",
    "                \"Pain behind the eyes\",\"pain in eyes\",\n",
    "                \"Severe joint and muscle pain\",\"joint and muscle pain\",\n",
    "                \"Fatigue\",\n",
    "                \"vomiting\",\"vomit\",\n",
    "                \"mild bleeding\",\"nose bleeding\"\n",
    "\n",
    "               \n",
    "               \n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendPatch(prevSent,lst):\n",
    "    for i in lst:\n",
    "        if i not in prevSent:\n",
    "            \n",
    "            prevSent = prevSent + \" , \"+i+\" , \"\n",
    "    prevSent = prevSent.rstrip(\", \")\n",
    "    return prevSent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    #text=text.lower()\n",
    "    text=re.sub(r\"i'm\",\"i am\",text)\n",
    "    text=re.sub(r\"I'm\",\"I am\",text)\n",
    "    text=re.sub(r\"he's\",\"he is\",text)\n",
    "    text=re.sub(r\"He's\",\"He is\",text)\n",
    "    text=re.sub(r\"she's\",\"she is\",text)\n",
    "    text=re.sub(r\"She's\",\"She is\",text)\n",
    "    text=re.sub(r\"that's\",\"that is\",text)\n",
    "    text=re.sub(r\"That's\",\"That is\",text)\n",
    "    text=re.sub(r\"what's\",\"what is\",text)\n",
    "    text=re.sub(r\"What's\",\"What is\",text)\n",
    "    text=re.sub(r\"where's\",\"where is\",text)\n",
    "    text=re.sub(r\"Where's\",\"Where is\",text)\n",
    "    \n",
    "    text=re.sub(r\"\\'ll\",\" will\",text)\n",
    "    text=re.sub(r\"\\'s\",\" is\",text)\n",
    "    text=re.sub(r\"\\'m\",\" am\",text)\n",
    "    \n",
    "    text=re.sub(r\"\\'ve\",\" have\",text)\n",
    "    text=re.sub(r\"\\'re\",\" are\",text)\n",
    "    text=re.sub(r\"\\'d\",\" would\",text)\n",
    "    text=re.sub(r\"won't\",\" will not\",text)\n",
    "    text=re.sub(r\"can't\",\" can not\",text)\n",
    "    text=re.sub(r\"[-()\\\"#$/?|.<>@:;+=,]\",\"\",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Ravi'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"I'm Ravi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent : Hi !! I am your Medical Assistant . My name is MEDANT. Say stop to stop chatting\n",
      "User : Hi\n",
      "Agent :  Hi . What's your name ?\n",
      "User : Swastik Gupta\n",
      "Agent : Hi Swastik ! How can I help you ..\n",
      "User : I have been having itchiness and weakness for some days\n",
      "Agent : I think that you may have dengue but i need more specifics apologies\n",
      "User : My urine has turned yellowish\n",
      "Agent : I think that you may have jaundice but i need more specifics\n",
      "User : stop\n",
      "Quitting .. Have a nice day !!!\n"
     ]
    }
   ],
   "source": [
    "user_data={}\n",
    "bag=[]\n",
    "print(\"Agent : \"+\"Hi !! I am your Medical Assistant . My name is MEDANT. Say stop to stop chatting\")\n",
    "while True:\n",
    "    \n",
    "    a=input( 'User : ' )\n",
    "    if a == \"stop\":\n",
    "        print(\"Quitting .. Have a nice day !!!\")\n",
    "        break\n",
    "\n",
    "        \n",
    "    a_clean=clean_text(a)\n",
    "    #print(a_clean)\n",
    "    a_1=a_clean.lower()\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    if a_1 in [\"hi\",\"hi there\",\"hello\",\"hola\",\"well hello there\",\"well , hello there\"]:\n",
    "        print( \"Agent : \"+ \" Hi . What's your name ?\" )\n",
    "        continue\n",
    "    \n",
    "    if a_1 in [\"thanks\",\"thank you\",\"thankyou\",\"thanks a lot\",\"thankyou very much\",\"thank you very much\",\"thanks for the help\"]:\n",
    "        print( \"Agent : \"+ \" No problem . What else can I do for you ?\" )\n",
    "        continue\n",
    "    \n",
    "    s=solve(a_clean)\n",
    "    if s!=\"\":\n",
    "        user_data[\"name\"]=s\n",
    "        print(\"Agent : Hi \"+ s + \" ! How can I help you ..\")\n",
    "        continue\n",
    "        \n",
    "    bag.extend(extractSymptoms(a_1,listOfSymptoms))\n",
    "    a_1 = appendPatch(a_1,bag)\n",
    "    \n",
    "    ls=a_1.strip().split()\n",
    "    \n",
    "    for i in ls :\n",
    "        if i not in list(dic.keys()):\n",
    "            a_1=a_1.replace(i,\" \")\n",
    "            \n",
    "    states_values = enc_model.predict( str_to_tokens(a_1 ) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "    deco=decoded_translation.strip('end')\n",
    "    \n",
    "    print( \"Agent : \"+ deco.strip().capitalize() )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample login demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    \n",
    "    print(\"Do you have an account .. (Y/n)\")\n",
    "    a=input().lower()\n",
    "    if a == \"n\":\n",
    "        print(\"Sign up to access our assistant ... \")\n",
    "        print(\"Pick a username ..\")\n",
    "        us_name=input()\n",
    "        print(\"Password please\")\n",
    "        password=input()\n",
    "        if us_name not in list(users.keys()):\n",
    "            users[us_name]=password\n",
    "        else:\n",
    "            print(\"Sorry but it is taken \")\n",
    "\n",
    "\n",
    "\n",
    "    elif a == \"y\":\n",
    "        print(\"Give your Username\")\n",
    "        us_name=input()\n",
    "        print(\"Password\")  \n",
    "        password=input()\n",
    "        if us_name not in list(users.keys()):\n",
    "            print(\"Please sign up\")\n",
    "        elif users[us_name]==password:\n",
    "            print(\"Validated Successfully\")\n",
    "            break\n",
    "        elif users[us_name] != password:\n",
    "            print(\"Wrong password\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Typo .. \")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
